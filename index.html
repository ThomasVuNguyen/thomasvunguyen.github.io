Hi, I'm Thomas.
<br>
Displayed below are the speed benchmark for luna inference, automatically generated every deployment

<pre>

</pre>
<pre>
| model                          |       size |     params | backend    | threads |          test |                  t/s |
| ------------------------------ | ---------: | ---------: | ---------- | ------: | ------------: | -------------------: |
| llama 1B Q8_0                  |   1.22 GiB |     1.24 B | CPU        |       4 |         pp512 |        105.77 ± 0.52 |
| llama 1B Q8_0                  |   1.22 GiB |     1.24 B | CPU        |       4 |         tg128 |         35.91 ± 0.45 |

build: 6fd7557 (1)
</pre>
