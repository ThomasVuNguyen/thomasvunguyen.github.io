Hi, I'm Thomas.
<br>
Displayed below are the speed benchmark for luna inference, automatically generated every deployment

Prompt:
Tell me a fun fact
<pre>
Prompt Output:

Benchmark Results:
</pre>
<pre>
Benchmark Results:
| model                          |       size |     params | backend    | threads |          test |                  t/s |
| ------------------------------ | ---------: | ---------: | ---------- | ------: | ------------: | -------------------: |
| llama 1B Q8_0                  |   1.22 GiB |     1.24 B | CPU        |       4 |         pp512 |        106.95 ± 0.37 |
| llama 1B Q8_0                  |   1.22 GiB |     1.24 B | CPU        |       4 |         tg128 |         36.60 ± 0.99 |

build: 610bd5e (1)
</pre>
